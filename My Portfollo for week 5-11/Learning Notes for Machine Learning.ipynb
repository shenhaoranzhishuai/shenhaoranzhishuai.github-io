{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Notes for Deep Dive Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 my GUID : 2719097S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 my GitHub repository URL:https://github.com/shenhaoranzhishuai/shenhaoranzhishuai.github.io.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Selected lab exercises : option2:Generating Text with Neural Networks\n",
    "#### 3-1: Getting the Data\n",
    "The code below utilizes TensorFlow to download a text file containing the works of Shakespeare from a specified URL, \"https://homl.info/shakespeare.\" The `get_file` function is employed to retrieve the file, saving it locally as \"shakespeare.txt.\" Subsequently, the script opens this local file and reads its content into the variable `shakespeare_text` using a `with` statement.  And display the first 80 characters of the content stored in the variable shakespeare_text. This is a quick way to inspect a snippet of the text and get a glimpse of the structure and style of the Shakespearean works that have been loaded into the program. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "shakespeare_url = \"https://homl.info/shakespeare\"  # shortcut URL\n",
    "filepath = tf.keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shakespeare_text[:80]) # not relevant to machine learning but relevant to exploring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-2: Preparing the Data\n",
    "In the code snippet below, a `TextVectorization` layer from TensorFlow is employed for converting the text data into numerical vectors. The `split` parameter is set to \"character,\" indicating that the text should be split into individual characters. The `standardize` parameter is set to \"lower,\" which means that the text will be converted to lowercase for uniformity. The `adapt` method is then called on the `text_vec_layer` with the provided Shakespearean text, allowing the layer to analyze the text and adapt its internal state accordingly.\n",
    "\n",
    "After adaptation, the `text_vec_layer` is used to encode the Shakespearean text into numerical vectors using the `([shakespeare_text])[0]` statement. Finally, the encoded result is printed with the line `print(text_vec_layer([shakespeare_text]))`. This line shows the numerical representation of the text, where each character is mapped to a unique numerical identifier. This vectorization is a common preprocessing step in natural language processing tasks, enabling the use of text data in machine learning models that require numerical input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vec_layer = tf.keras.layers.TextVectorization(split=\"character\",\n",
    "                                                   standardize=\"lower\")\n",
    "text_vec_layer.adapt([shakespeare_text])\n",
    "encoded = text_vec_layer([shakespeare_text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_vec_layer([shakespeare_text]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code segment below adjusts the encoded text by subtracting 2, eliminating tokens 0 (pad) and 1 (unknown) which are not used. The variable `n_tokens` is then calculated, representing the number of distinct characters in the text (excluding the special tokens). Additionally, `dataset_size` is determined, representing the total number of characters in the encoded text. The printed values of `n_tokens` and `dataset_size` provide insights into the diversity of characters and the overall size of the processed text, which can be crucial for configuring model parameters in subsequent tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded -= 2  # drop tokens 0 (pad) and 1 (unknown), which we will not use\n",
    "n_tokens = text_vec_layer.vocabulary_size() - 2  # number of distinct chars = 39\n",
    "dataset_size = len(encoded)  # total number of chars = 1,115,394"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_tokens, dataset_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below defines a function `to_dataset` that converts a sequence of data into a TensorFlow Dataset suitable for training machine learning models. The function takes several parameters:\n",
    "\n",
    "- `sequence`: The input sequence of data.\n",
    "- `length`: The length of the sequences to create for training. It is used to create overlapping windows of data.\n",
    "- `shuffle`: A boolean indicating whether to shuffle the data.\n",
    "- `seed`: Seed for reproducibility if shuffling is enabled.\n",
    "- `batch_size`: The size of the batches in the resulting dataset.\n",
    "\n",
    "The function creates a TensorFlow Dataset from the input sequence, windows it into overlapping sequences of the specified length, and optionally shuffles the data. It then batches the data and returns a dataset where each element is a tuple of two parts: the input sequence (`window[:, :-1]`) and the target sequence (`window[:, 1:]`). The last line prefetches one batch to improve data loading performance.\n",
    "\n",
    "The code then uses this function to create three datasets (`train_set`, `valid_set`, and `test_set`) from the encoded Shakespearean text. The `train_set` is shuffled, and all three datasets have sequences of length 100. These datasets can be used for training, validation, and testing machine learning models, particularly those designed for sequence processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "    ds = ds.window(length + 1, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda window_ds: window_ds.batch(length + 1))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(100_000, seed=seed)\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 100\n",
    "tf.random.set_seed(42)\n",
    "train_set = to_dataset(encoded[:1_000_000], length=length, shuffle=True,\n",
    "                       seed=42)\n",
    "valid_set = to_dataset(encoded[1_000_000:1_060_000], length=length)\n",
    "test_set = to_dataset(encoded[1_060_000:], length=length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-3: Building and Training the Model\n",
    "The code below establishes a reproducible environment (`tf.random.set_seed(42)`) and constructs a sequential TensorFlow model for Shakespearean text generation. Key model components include an embedding layer (`Embedding`) with input dimension `n_tokens` and output dimension 16, a GRU layer (`GRU`) with 128 units returning sequences, and a dense layer (`Dense`) with a softmax activation function. This model is compiled with a sparse categorical cross-entropy loss function, the Nadam optimizer, and accuracy as the metric. The training process (`model.fit`) is executed on a training set (`train_set`) with validation on a separate set (`valid_set`). The trained model is then saved to a file (\"myModel.h5\"). Furthermore, a combined model (`shakespeare_model`) is created by incorporating a text vectorization layer (`text_vec_layer`) and the trained text generation model. This combined model allows inputting raw text and generating predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n",
    "    tf.keras.layers.GRU(128, return_sequences=True),\n",
    "    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=20,\n",
    "                    steps_per_epoch=1000)\n",
    "model.save('./my_shakespeare_model/myModel.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `model = tf.keras.Sequential` ,we can also change the network structure by modifying the `GRU` of `tf.keras.layers.GRU(128, return_sequences=True)` into  `LSTM` or `RNN` et al. Different training effects can be obtained through different networks to obtain the most suitable network for the needs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_model = tf.keras.Sequential([\n",
    "    text_vec_layer,\n",
    "    tf.keras.layers.Lambda(lambda X: X - 2),  # no <PAD> or <UNK> tokens\n",
    "    model\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-4: Generating Text\n",
    "The code below use the trained combined model (`shakespeare_model`) to predict the next character in a given input sequence, \"To be or not to b.\"\n",
    "\n",
    "- `y_proba = shakespeare_model.predict([\"To be or not to b\"])[0, -1]`: This line generates the probability distribution for the next character in the sequence by predicting with the model. The `[0, -1]` indexing is used to access the last predicted character in the sequence.\n",
    "\n",
    "- `y_pred = tf.argmax(y_proba)`: Here, the character with the highest probability is chosen by finding the index of the maximum value in the probability distribution.\n",
    "\n",
    "- `text_vec_layer.get_vocabulary()[y_pred + 2]`: Finally, the vocabulary of the text vectorization layer is used to map the predicted character ID (`y_pred + 2`) back to the actual character.\n",
    "\n",
    "In summary, these lines demonstrate how to use the trained model to predict the next character in a sequence and convert the predicted character ID back to the corresponding character using the vocabulary of the text vectorization layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = shakespeare_model.predict([\"To be or not to b\"])[0, -1]\n",
    "y_pred = tf.argmax(y_proba)  # choose the most probable character ID\n",
    "text_vec_layer.get_vocabulary()[y_pred + 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probas = tf.math.log([[0.5, 0.4, 0.1]])  # probas = 50%, 40%, and 10%\n",
    "tf.random.set_seed(42)\n",
    "tf.random.categorical(log_probas, num_samples=8)  # draw 8 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions below facilitate text generation using the trained Shakespearean model:\n",
    "\n",
    "1. **`next_char(text, temperature=1)`:**\n",
    "   - Predicts the next character in the input `text` using the trained model.\n",
    "   - `temperature` controls the diversity of predictions: higher values make predictions more diverse.\n",
    "   - `y_proba` stores the probability distribution for the next character, and `rescaled_logits` adjusts the distribution based on the temperature.\n",
    "   - The final character ID is randomly sampled, converted to the actual character using the text vectorization layer's vocabulary, and returned.\n",
    "\n",
    "2. **`extend_text(text, n_chars=50, temperature=1)`:**\n",
    "   - Extends the input `text` by iteratively predicting and appending the next character.\n",
    "   - Calls the `next_char` function for each iteration, allowing for the generation of a sequence of characters.\n",
    "   - `n_chars` determines the length of the generated text.\n",
    "\n",
    "3. **Reproducibility Setup:**\n",
    "   - `tf.random.set_seed(42)`: Sets the random seed to 42 for reproducibility.\n",
    "\n",
    "4. **Text Generation Examples:**\n",
    "   - Generates extended text based on different temperature settings, influencing the diversity of the generated text. Lower temperatures produce more focused output, while higher temperatures result in more diverse and random text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    y_proba = shakespeare_model.predict([text])[0, -1:]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0, 0]\n",
    "    return text_vec_layer.get_vocabulary()[char_id + 2]\n",
    "\n",
    "def extend_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text\n",
    "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extend_text(\"To be or not to be\", temperature=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extend_text(\"To be or not to be\", temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extend_text(\"To be or not to be\", temperature=100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "last",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
